\section{Evaluation Comparison}\label{sec:result_comparison}
In the final evaluation step we take a broader look on the overall performance of context enrichment. For that, we combined the results from each dataset. This has the advantage of reducing the sensibility to a particular dataset which is recommended for unbiased interpretations. 

Based on our initial hypothesis which motivates context enrichment, we formulated a couple of questions that were answered next:
\paragraph{Which Context Enrichment Method performed best in general?}
Our observations confirmed our initial hypothesis which suggests extending basic crowd-based ontology validation with context. From the combined results of all datasets as shown in~\hyperref[table:bench_p_r_f_combined]{Table~\ref*{table:bench_p_r_f_combined}} it is evident that embedded context worked best. In fact, it had not only the highest value of F-Measure but also the highest precision and recall. Indeed, this was rather expected due to the fact that context being manually added. Obviously no one has a better domain knowledge than the creators or maintainers of the ontology. 
On the bottom end of the table is the existing approach with missing context. 
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{table}
	\begin{tabularx}{\textwidth}{l c*{3}{Y}}
		\toprule
		Method & Precision & Recall & F-Measure \\
		\midrule
		 Embedded Context & 0.797 & 0.921 & 0.854 \\
		 Neighbouring Nodes & 0.787 & 0.887 & 0.834 \\
		 External Source & 0.729 & 0.899 & 0.805 \\
		 None & 0.674 & 0.910 & 0.775 \\
		\bottomrule
	\end{tabularx}
	\caption{Aggregated results of all datasets~(ranked by F-Measure)}
	\label{table:bench_p_r_f_combined}
\end{table}
\endgroup

\paragraph{Did the crowd perform better with context?}
\hyperref[fig:results_accuracy_combined]{Figure~\ref*{fig:results_accuracy_combined}} depicts the combined accuracy of all methods which is represented
by the ratio of correct and incorrect judgements. For easier comparability, the exact number of judgements was written in labels. The performance of the top ranked method~(Embedded Context) is quite impressive. Concepts were judged correctly for nearly eighty percent~($78.4\%$), being an improvement of over $14\%$ compared to omitting concept descriptions. Even for the least ranked method~(External Source), judgements were $4.6\%$ more accurate.   
\begin{figure}
	 \centering
	 \includegraphics[width=0.75\textwidth]{plots/comparison/barplot_all_judgements}
	 \caption{Combined accuracy of crowdsourcing methods}\label{fig:results_accuracy_combined}
\end{figure}

\paragraph{For which concepts were the crowd wrong?}
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{table}
	\begin{tabularx}{\textwidth}{l c*{4}{Y}}
		\toprule
		\multirow{2}{*}{\emph{Concept}} & \multicolumn{4}{c}{\emph{Methods}} & \emph{Accuracy}\\
		\cmidrule(lr){2-5} \cmidrule(lr){6-6} 
		 & EC & NN & ES & NONE & Total\\
		\midrule
		sceptic & 0/5 & 0/5 & 0/5 & 0/5 & 0/20 \\
		greenhouse & 0/5 & 1/5 & 0/5 & 0/5 & 1/20 \\
		pipeline & 0/5 & 0/5 & 1/5 & 0/5 & 1/20 \\
		consensus & 2/5 & 0/5 & 0/5 & 0/5 & 2/20 \\
		denier & 2/5 & 0/5 & 0/5 & 0/5 & 2/20 \\
		production & 1/5 & 1/5 & 0/5 & 0/5 & 2/20 \\
		\bottomrule
	\end{tabularx}
	\caption{Concepts where most crowd workers had problems~(EC=Embedded Context, NN=Neighbouring Nodes, ES=External Source, NONE=No Context)}
	\label{table:bench_p_r_f_combined}
\end{table}
\endgroup

Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\paragraph{What other observations were found?}
A general phenomenon of all approaches was the exceptionally high value of precision indicating that the crowd tends to rather reject concepts in case 
of uncertainty or lack of knowledge. 