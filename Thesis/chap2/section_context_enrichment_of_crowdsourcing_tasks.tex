\section{The use of Context in Crowdsourcing Tasks}
% Notes from meeting with MS:
% 
% -) In related work, consider the use of context (e.g. define what "Context" means --> see definition from MS)
% -) Use table with references provided by MS

\subsection{Context}
When analysing the use of context in crowdsourcing tasks, we noticed that there exists no formal definition of the term \guillemotright context\guillemotleft~. In fact, all approaches that were found use a different notion of that term. 
This section first investigates what context definitions these approaches use. After that, we give a consolidated definition that fits our approach of crowd-based ontology validation.

When investigating the use of context in crowdsourcing tasks, a good start is to look at~\cite{sarasua2015crowdsourcing}. In this work, the authors did an extensive literature study to find challenges in the context of Crowdsourcing and the Semantic Web. One of the challenges they found was a proper definition of context as part of a complete task design. Concretely, they asked but did not answer the minimum required context a crowd needs to finish a task correctly. Unfortunately, during our studies we could not find an answer either. It seems that there exists no generic answer which applies in all contexts, it rather depends on the concrete type of task that needs to be solved. 

During our literature research we found that approaches can be categorised as tasks supplying \textbf{explicit context}, tasks supplying \textbf{implicit context} and those providing \textbf{no context} at all. 

The most obvious work supplying \emph{no~context} with crowdsourcing tasks was actually done by~\cite{wohlgenannt2016}. It represents the baseline of our work and motives our use of context. A detailed explanation of this paper is out of scope for this section. However, a detailed explanation was already done in~\hyperref[sec:ucomp_protege_plugin]{Section~\ref*{sec:ucomp_protege_plugin}}. In another paper, a method of collaborative ontology construction was proposed~\cite{zhitomirsky2017}. The actual definition of the ontology was implemented by a hybrid approach containing the definition of RDF-triples by non-experts~(e.g. students) and their classification by the crowd.

Clearly, the omission of context does not need to be problematic. Whereas crowd-based ontology validation without context clearly has its drawbacks, it would not be beneficial if the crowd had additional information in the ontology construction example because the entities that formed the statements that were judged were simple and easy understandable by the crowd. 

The other group of tasks that provide additional information are those tasks supplying \emph{explicit~context}. Similarly to our approach the authors of \cite{mortensen2015} and \cite{mortensen2016} supplied concept descriptions to improve the quality of the judgements. Their goal was to find inconsistencies and errors in SNOMED~CT, a widely used ontology mainly used in biomedical contexts. Even though biomedical ontologies are well documented, not all entities have definitions. For that, English language definitions were manually added by domain experts. 

The last category contains tasks with \emph{implicit~context}, meaning that context was not intentionally added. Context is rather defined implicitly, e.g. all context is already present in the initial dataset. Hence, no additional process or algorithm is needed to define contextual information. For example, in~\cite{acosta2018} the authors used a crowdsourcing data quality assessment tool to detect errors in Linked Data. 
For their analysis they used DBPedia~\cite{auer2007} as evaluation source. Because one of the design principles of DBPedia was to derive linked information from Wikipedia\footnote{\url{https://www.wikipedia.org/}}, it seems natural to add the link to the corresponding Wikipedia page to the crowdsourcing task interface. To that end, no additional process or method for the enrichment of context exists because the evaluated dataset already contains the context.

A different approach was taken by~\cite{sabou2018, winkler2017, winkler2017_2} in which an Extended Entity Relationship~(EER)~diagram was verified against a software specification document in a software engineering use case. The diagram, initially created by students, was presented together with the specification text to detect and correct inconsistencies in the conceptual model. From the task description it seems obvious that no additional information is needed because context was already given implicitly by the EER~diagram. In that sense, the context was defined in terms the specific task that was carried out by the crowd. 

Taken together all insights from above, we define \guillemotright context\guillemotleft~in crowdsourcing tasks as:

\begin{defn}
	\emph{(Context)} Context refers to any sort of additional information that is supplied with a crowdsourcing task to improve its understanding in
	such a way that it positively affects the crowds performance and the result quality. Furthermore, we do not set a limitation on the type or format 
	of context that is provided. Examples are natural language descriptions, links to external content or pictures. We distinguish between
	crowdsourcing tasks that
	\begin{inparaenum}[1)]
			\item supply explicit context,
			\item those that supply context implicitly and
			\item those that provide no context at all.
	\end{inparaenum}
\end{defn}



\subsection{Approaches that use Context for Ontology Validation}
% See table from MS of existing apporoaches that use Context in Human Computation %
