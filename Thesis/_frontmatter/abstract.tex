\begin{abstract}
	
	% ROADMAP for the Abstract (1 page):
	% 1st paragraph: Problem statement																		DONE
	% 2ns paragraph: Motivation for writing the thesis														DONE
	% 3rd paragraph: The contributions of the thesis (e.g. the proposed approaches)							DONE
	% 4th paragraph: Evaluation (used datasets, RESULTS --- outcome of the evaluation part)					DONE
	
	
	Validating the relevance of ontologies is considered an important task in the Semantic Web Lifecycle. This holds especially for 
	learned ontologies which contain quite naturally a lot of errors. Although many errors can be solved algorithmically, solving more complex problems
	by machines becomes very hard if not impossible. Crowdsourcing offers a cost effective alternative in which tasks were solved by a large group of
	human workers. However, the performance of existing approaches that combine ontology validation with Crowdsourcing is still not satisfying.
	
	A promising way of tackling this problem is to enrich Crowdsourcing tasks with additional information to improve its understanding. 
	This \emph{Context} has not only a positive impact on the crowd's performance but also raises the result quality. 
	
	Even though recent research showed advances in this area, the use of Context was not explicitly targeted. In this thesis we present three novel
	methods that enrich Crowdsourcing tasks with contextual information to validate the relevance of concepts for a particular domain of interest.
	Whereas the Ontology~based~Approach processes hierarchical relations, the Metadata~based~Approach generates descriptions based on annotations that
	were encoded within the ontology, the idea of the Dictionary~based~Approach is to form the explanations from example sentences by conducting the
	online dictionary WordNik.
	
	For the analysis of all three approaches, we integrated these approaches into the existing uComp Protege Plugin which facilitates the
	integration of crowdsourcing tasks for ontology validation from within the Protege ontology editor. The evaluation was performed on three
	ontologies covering the domains of climate change, tennis and finance. For each dataset, the performance metrics Precision, Recall and F-Measure
	were calculated to compare the methods against the existing approach. 
	The results showed that the Metadata~based~Approach outperformed all other methods. The other two approaches had some difficulties in certain
	situations, for example the Dictionary~based~Approach sometimes added inappropriate explanations, especially for concepts with multiple meanings
	associated. Likewise, the Ontology~based~Approach had problems with loosely connected ontologies containing just a few subsumption relations.
	However, all three approaches delivered results of higher quality than omitting descriptions, indicating that Context in Crowdsourcing tasks
	is a cost-effective alternative of improving the performance of the crowd. 
	
\end{abstract}