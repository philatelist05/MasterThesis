\paragraph{Searching in Electronic Dictionaries} Searching is probably the most important feature of today's electronic dictionaries. In the next paragraphs we discuss problems that arise when searching for specific terms and strategies to overcome these. 

Several studies~\cite{pastor2010, mvechura2008} have analysed digital dictionaries, available in different languages and accessible via the Internet or compressed on a digital medium~(e.g. CD-ROM). Their evaluation is based on empirical analyses of user behaviour collected from log-files as well as literature review. All searching techniques listed below tackle the problem of invalid, incomplete, misspelled and multi-word search queries.

The first category are \textit{inflection-aware} search algorithms which accept inflected and uninflected search queries. The central part of these algorithms are rewrite rules which transform the input query to the matching headword or vice-versa. A powerful but complicated technique is based on a deep understanding of the target language. It is based on morphologies of the word endings. For example, stripping \emph{-ed} and \emph{-ing} endings for regular verbs is yet simple but only manageable for regular verbs, more complicated algorithms are needed for irregular verbs. Other algorithms, not taking the structure of the target language into account, are based on fuzzy string searching, a technique based on approximate string matching. A simple metric measuring the difference between two strings is the Levenshtein~Distance~\cite{levenshtein1966}. It is defined as the number of single character manipulations necessary to transform one string into the other. 

The next category are algorithms dealing with \textit{multi-word} search items. Search algorithms need to deal with user entered search strings, containing multiple words or even complete sentences. The situation is even more complicated, for a single-word search query users expect not only matching single-word entries but also matching multi-word entries~(e.g. phrasal verb). Enumerating all possible word combinations might be acceptable in dictionaries with small corpora, but it is infeasible in dictionaries with millions of entries and performance is important. In these cases it is better to construct an index from single-word entries and multi-word entries. An even better approach, also known by the term \emph{lemmatising}, would be to analyse and process the words before-hand.

Another challenge in processing search queries is \textit{detecting misspellings}. User input is inherently prone to errors. Smart algorithms guide the user to the intended word by offering corrections and suggestions, similar to what a spellchecker does. A naive approach would be to collect lists of common misspelled words and continue with matching. This has the disadvantage that not all possible combinations are included and processing takes longer, especially for long word lists. A better but yet more complicated technique is based on word similarity which is discussed in the previous paragraph. 

A technique especially useful for multi-lingual dictionaries is \textit{language selection and detection}. As there already exists much literature in this area~\cite{mcnamee2005, lodhi2002, vatanen2010, selamat2016}, we decided to just outline two approaches here. The first group of algorithms is purely based on statistical models. The detection of the target language works by building probabilistic models of the inferred statistics and the pre-trained statistics. The second group works by counting common words or character sequences~(called n-grams) and comparing the obtained frequency counts against reference values~\cite{mcnamee2005}.

The last technique is \textit{text normalisation}. It solves the problem of unwanted characters in search strings by removing variation and reducing words to simpler formats. For search strings, this means that noisy characters are removed. A simple strategy is to delete characters which do not add any meaning such as hyphens, dots quotation marks and (mathematical)~symbols. Additionally, consecutive occurrences of whitespaces are merged. 
The normaliser also need to consider some character variations as defined by the Unicode~Standard\footnote{\url{http://www.unicode.org/reports/tr15/} accessed 2018/06/08 }. 

