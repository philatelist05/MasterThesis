\begin{kurzfassung}
	Ein wichtiger Bestandteil des Semantik Web Lebenszyklus ist die Überprüfung der Ontologie Relevanz. Dies ist insbesondere der Fall bei erlernten 
	Ontologien, welche von Natur aus viele Fehler enthalten. Obwohl viele Fehler von Algorithmen gelöst werden, kann dies mitunter bei komplexeren
	Problemstellungen schwierig sein. Crowdsourcing stellt eine kosteneffiziente Alternative dar diese Problemstellungen mit menschlicher Kraft löst. 
	Dennoch ist die Performance jener Ansätze die Crowdsourcing mit Ontologie Validierung verknüpft nicht zufriedenstellend. 
	
	Ein vielversprechender Ansatzpunkt dieses Problem zu lösen wäre, wenn die Crowdsourcing Aufgaben zusätzliche Informationen zur besseren
	Verständlichkeit enthalten würden. Dieser Kontext hätte nicht nur einen positiven Einfluss auf die Performance der Teilnehmer, sondern würde auch 
	zur besseren Qualität der Ergebnisse beitragen.
	
	Obwohl vielversprechende Ergebnisse in diesem Bereich kürzlich publiziert wurden, befasste sich keine der Publikationen mit dem Thema Kontext im
	Zusammenhang mit Crowdsourcing. In dieser Diplomarbeit präsentieren wir 3 neuartige Methoden die Kontext für Crowdsourcing Aufgaben herstellen um
	die Relevanz von Konzepten für eine bestimmte Domäne zu überprüfen. Während die auf Ontologien basierende Methode hierarchische Relationen
	verarbeitet, generiert die auf Metadaten basierende Methode Beschreibungen welche auf Annotationen beruhen. Durch Suchabfrage über die Platform
	WordNik	werden Beispielsätze geformt, welche die Basis für die dritte Methode darstellen. 
	
	
	%For the analysis of all three approaches, we integrated these into the existing uComp Protege Plugin which facilitates the
	%integration of crowdsourcing tasks for ontology validation from within the Protege ontology editor. The evaluation was performed on three
	%ontologies covering the domains of climate change, tennis and finance. For each dataset, the performance metrics Precision, Recall and F-Measure
	%were calculated to compare the methods against the existing approach. 
	%The results showed that the Metadata~based~Approach outperformed all other methods. The other two approaches had some difficulties in certain
	%situations, for example the Dictionary~based~Approach sometimes added inappropriate explanations, especially for concepts with multiple meanings
	%associated. Likewise, the Ontology~based~Approach had problems with loosely connected ontologies containing just a few subsumption relations.
	%However, all three approaches delivered results of high quality, indicating that adding Context to Crowdsourcing tasks
	%is a cost-effective method of improving the crowd's performance.
	
	
	
\end{kurzfassung}