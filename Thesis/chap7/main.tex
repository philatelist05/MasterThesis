\chapter{Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Data Collection %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1.) Take the described 3 ontologies																								DONE
%2.) Check the annotations (whether they are relevant in the respective domain or not)												DONE
%3.) Check and manually add context descriptions (as described using the SKOS metadata set)											DONE
%4.) Alter the implementation of "Embedded context" to use the SKOS metadata set instead of the OBO metadata set					DONE
%5.) Run the Plugin to generate Figure Eight tasks (IMPORTANT: TAKE SCREENSHOTS FOR EACH OF THE CONTEXT ENRICHMENT METHODS)			DONE
%6.) Also take screenshots when receiving the judgements from Figure Eight															DONE
%7.) Download and collect the raw set of judgements																					DONE	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Data Visualisation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%1.) Run experiments with context enrichment and without context enrichment															DONE
	%2.1) Calculate overall Precision, Recall, F-measure and Agreement ratio														DONE
	%2.2) Calculate metrics for each question (to show outliers)																	DONE
	%2.3) Calculate metrics for each crowd worker (to show spammers)																DONE
	%2.3) Draw plots																												DONE

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Based on the metrics discussed in \hyperref[sec:evaluation_metrics]{Section~\ref*{sec:evaluation_metrics}}, this chapter highlights the results of the context enrichment methods.

This chapter is divided along the used datasets for evaluation, that is, \hyperref[sec:result_cc_ontology]{Section~\ref*{sec:result_cc_ontology}} covers the \emph{climate~change} domain, \hyperref[sec:result_f_ontology]{Section~\ref*{sec:result_f_ontology}} is dedicated to \emph{finance} and \emph{tennis} is handled in \hyperref[sec:result_t_ontology]{Section~\ref*{sec:result_t_ontology}}. Finally, an overall \emph{comparison} of all evaluation domains is done in \hyperref[sec:result_comparison]{Section~\ref*{sec:result_comparison}}.


\section{Climate Change Ontology}\label{sec:result_cc_ontology}
In this section, results from the crowdsourced ontology validation in the field of climate change are presented. A detailed discussion of the ontology used as a baseline for all calculations is done in \hyperref[sec:evaluation_datasets]{Section~\ref*{sec:evaluation_datasets}}. 

The results of the benchmark are presented in~\hyperref[table:bench_p_r_f]{Table~\ref*{table:bench_p_r_f}}. For comparison, we also performed ontology validation without any of the discussed context enrichment methods~(indicated as \emph{None}). Given the relatively small number of concepts, any of the context enrichment method perform better than having no context at all. Surprisingly, in terms of recall the contrary holds. Indeed, crowd workers tend to positively answer questions in case of uncertainty when no additional information other than the concept name is present. 
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{table}
	\begin{tabularx}{\textwidth}{l c*{3}{Y}}
		\toprule
		Method & Precision & Recall & F-Measure \\
		\midrule
		 Neighbouring Nodes & 0.758 & 0.805 & 0.781 \\
		 Embedded Context & 0.732 & 0.831 & 0.778 \\
		 External Source & 0.724 & 0.821 & 0.769 \\
		 None & 0.549 & 0.837 & 0.663 \\
		\bottomrule
	\end{tabularx}
	\caption{Aggregated results on the Climate Change Ontology~(ranked by F-Measure)}
	\label{table:bench_p_r_f}
\end{table}
\endgroup

We also measured the agreement ratio~(inter-rater agreement) in this dataset. \hyperref[fig:hist_agreement_all]{Figure~\ref*{fig:hist_agreement_all}} shows the distribution of the agreement ratio among all validated concepts. We required 5 judgements for every concept, giving $5/0$, $4/1$ or $3/2$ levels of agreement, which is equivalent to $1.0$, $0.8$ and $0.6$ respectively. 

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{plots/hist_agreement_nn}
        \caption{Neighbouring Nodes}
        \label{fig:hist_agreement_nn}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{plots/hist_agreement_ec}
        \caption{Embedded Context}
        \label{fig:hist_agreement_ec}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{plots/hist_agreement_es}
        \caption{External Source}
        \label{fig:hist_agreement_es}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{plots/hist_agreement_none}
        \caption{None}
        \label{fig:hist_agreement_none}
    \end{subfigure}
    \caption{Histogram plots of the Inter-rater Agreement}\label{fig:hist_agreement_all}
\end{figure}

The highest agreement exhibits \emph{External~Source}, followed by \emph{None}. This is somewhat interesting as these are the methods with the lowest performance with regard to F-measure. In fact, the agreement ratio just describes to what extent the responses of crowd workers coincide. From the observations in this dataset, it is hard, if not impossible, to draw conclusions solely based on agreement. In fact, when looking closely at the judgements with the highest agreement~ratio among incorrect answers, $16$ of $17$ judgements for External~Source and $3$ of $6$ judgements for Neighbouring~Nodes had context added. Apparently, crowd workers agreed here on incorrect values even though that concept descriptions were available. 

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{plots/hist_level_nn}
        \caption{Neighbouring Nodes}
        \label{fig:hist_level_nn}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{plots/hist_level_ec}
        \caption{Embedded Context}
        \label{fig:hist_level_ec}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{plots/hist_level_es}
        \caption{External Source}
        \label{fig:hist_level_es}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{plots/hist_level_none}
        \caption{None}
        \label{fig:hist_level_none}
    \end{subfigure}
    \caption{Histogram plots of the correct/incorrect judgements}\label{fig:hist_level_all}
\end{figure}

To manifest the observations from above, a bar-plot~(illustrated in \hyperref[fig:hist_level_all]{Figure~\ref*{fig:hist_level_all}}) was created. It combines the agreement ratio with the amount of correct/incorrect judgements. Whereas a negative score indicates that more contributors agreed on the incorrect answer, a positive score shows that the majority of crowd worker's responses were correct. Indeed, when comparing the performance on level $-5$ \emph{External~Source} is on the same level as the omission of context. On the other hand, it exhibits the highest score on correct answers of level $5$.

Given the plots on the distribution of the correct/incorrect judgements from above, \hyperref[table:level_corr_incorr]{Table~\ref*{table:level_corr_incorr}} shows the summary statistics concerning agreement level for each context enrichment method. It confirms the observations made so far.
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{table}
	\begin{tabularx}{\textwidth}{l c*{4}{Y}}
		\toprule
		Method & mean & median & $1^{st}$ quartile & $3^{rd}$ quartile \\
		\midrule
		 Neighbouring Nodes & 1.98 & 3.00 & 1.00 & 5.00 \\
		 Embedded Context & 2.04 & 3.00 & -1.00 & 5.00 \\
		 External Source & 1.92 & 5.00 & -1.00 & 5.00 \\
		 None & 1.04 & 1.00 & -3.00 & 5.00 \\
		\bottomrule
	\end{tabularx}
	\caption{Summary statistics concerning agreement level on the Climate Change Ontology}
	\label{table:level_corr_incorr}
\end{table}
\endgroup

\section{Finance Ontology}\label{sec:result_f_ontology}


\section{Tennis Ontology}\label{sec:result_t_ontology}


\section{Evaluation Comparison}\label{sec:result_comparison}



