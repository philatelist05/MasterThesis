\section{Aim of the Work}
When investigating the use of Context in Crowdsourcing tasks the first question that arise is, what is actually
meant by the term \guillemotright Context\guillemotleft~. Besides referring to the need of Context to improve the understanding
of Crowdsourcing tasks~\cite{sarasua2015crowdsourcing}, not much literature exist yet that exclusively targets this topic. An overview
of existing work as well as a conclusive definition of Context is given in~\hyperref[sec:the_use_of_context_in_crowdsourcing]{Section~\ref*{sec:the_use_of_context_in_crowdsourcing}}.

Hence, the following research questions that target the need of Context in Crowdsourcing tasks are addressed in this work:

\textbf{RQ-I} \emph{Does the crowd perform better on context enriched Crowdsourcing tasks?}

The basic question that motivates our research is whether the performance of crowd workers could be improved if Context was added
to Crowdsourcing tasks. Researchers have already stated this question~\cite{sarasua2015crowdsourcing} but not much research exists that relates to this topic. 

In order to give a detailed answer to this research question, a conclusive definition of \guillemotright Context\guillemotleft~ needs to be stated. While some work exists that uses Context, either implicitly or explicitly, no such definition exist yet. Whereas~\hyperref[sec:context_in_crowdsourcing_tasks_context]{Section~\ref*{sec:context_in_crowdsourcing_tasks_context}} gives a conclusive definition of \guillemotright Context\guillemotleft~, \hyperref[sec:context_in_crowdsourcing_tasks_approaches]{Section~\ref*{sec:context_in_crowdsourcing_tasks_approaches}} examines existing approaches that use Context in Crowdsourcing tasks. 

Answering this question directly leads to the next two research questions, our goal being to find generic methods applicable to similar
datasets:

\textbf{RQ-II} \emph{What methods can be applied that generate Context?}

In~\hyperref[chap:context_enrichment_methods]{Chapter~\ref*{chap:context_enrichment_methods}} we take a closer look into the approaches that generate Context suitable for extending Crowdsourcing tasks. Depending on how much manual intervention is required, Context was either generated automatically by an algorithm or manually by domain experts. 

Additionally, the methods were tested against three datasets~(e.g. ontologies) covering the domains of climate change, finance and tennis because an important goal was to measure the performance on a broader level. This leads to question which is stated next:
 
\textbf{RQ-III} \emph{To what extent is it possible to transfer the investigated methods to different datasets?}

One important design goal of all approaches was to remove any bias to a particular dataset. Hence, all the approaches were evaluated on three different datasets covering diverse domains such as finance, tennis and climate change. 

Furthermore, all datasets were combined to get a better picture on the performance of each method on average. To that end, we also analysed any characteristics of the used datasets to be able to make a statement on the generality of each approach. Based on this evaluation, we could answer the final research question:

\textbf{RQ-IV} \emph{Which of the proposed methods works best? What are potential shortcomings and why?}

Finally, we answer the question, which of our proposed methods work best on average and under which conditions. There may be restrictions on the applicability because of the reduced size and budget of our experiment. Indeed, some issues were found that are worth mentioning and leave room for future improvements. 



