\newacronym{gwap}{GWAP}{Games with a purpose}
\newacronym{mturk}{MTurk}{Amazon Mechanical Turk}

\section{Motivation}
The advance of embedding Information Technology in all kinds of electronic devices and connecting them to collect and exchange data imposes new challenges of handling the increasing amount of data. Although many problems can be solved by machines only, there are certain tasks where humans perform better than computers. In \emph{Crowdsourcing}, collective human intelligence is used to solve these complex tasks. \cite{yuen2011}~grouped Crowdsourcing applications in 
\begin{inparaenum}[1)]
		\item Voting Systems,
		\item Information Sharing Systems,
		\item \gls{gwap} Systems and
		\item Creative Systems.
\end{inparaenum} 
First, Voting Systems like \gls{mturk}\footnote{\url{https://www.mturk.com/}} use majority voting to consider the answer with the highest number of votes as the correct one. Second, Information Sharing Systems enable users sharing and distributing knowledge among the crowd. Third, \gls{gwap} Systems facilitate playing small games in order to solve some meaningful tasks. Fourth, Creative Systems include tasks like labelling an image, writing algorithms or editing text. 

An inherent factor of the Semantic Web is its large amount of Linked Data~(e.g. DBpedia~\cite{lehmann2015}). Semantic technologies have emerged in various areas including domain modelling, data integration, enhanced search and content management~\cite{semantic-web-usecases}. Managing Semantic Web tasks is considered resource intensive and often requires human involvement due to its knowledge intensive and context specific nature. On the other side Crowdsourcing applications solve simple and small tasks in a cost-effective way. \cite{sarasua2015crowdsourcing} summarises major research challenges and opportunities in combining Crowdsourcing and Semantic Web technologies. The most important challenges include 
\begin{inparaenum}[1)]
		\item task and workflow design,
		\item managing the quality of contributions,
		\item handling multiple Crowdsourcing genres and 
		\item finding and managing the right crowd.
\end{inparaenum}

Whereas research shows that breaking tasks into smaller pieces and formulating the right questions has a huge impact on the outcome of Crowdsourcing tasks, it is equally important to establish a model which formally defines the required quality and skills to solve tasks. Also, there exist no general guidelines when and under which circumstances preferring small crowds with domain experts over large crowds with less qualified crowd workers is better. However, \cite{mortensen2013} concluded that average crowds perform on par with domain experts in "common sense" application domains, if crowd workers are carefully selected by qualification tests and tasks are presented in the simplest possible form.


Clearly, in order to get the most qualitative responses from crowd workers, Crowdsourcing tasks need to provide enough Context that helps contributors to fully understand the task. This is especially important for ontology validation which requires expert knowledge. To support ontology engineers and domain experts, the uComp Protege Plugin~\cite{wohlgenannt2016} was developed which facilitates the creation and execution of Crowdsourcing tasks for ontology validation from within the Protege ontology editor. It supports the following the following tasks:
\begin{inparaenum}[1)]
		\item Verification of Domain Relevance,
		\item Verification of Relation Correctness,
		\item Specification of Relation Type and 
		\item Verification of Domain and Range.
\end{inparaenum}

In this thesis we extend the uComp Protege Plugin and present three approaches that add contextual information to Crowdsourcing tasks.
