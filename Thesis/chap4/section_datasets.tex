\newacronym{eurovoc}{EuroVoc}{Multilingual Thesaurus of the European Union}


\section{Datasets}\label{sec:evaluation_datasets}
Within the next paragraphs, ontologies used as the input for evaluation tasks are described in more detail. As this thesis builds on existing work~\cite{wohlgenannt2016}, it makes sense to use the same ontologies as evaluation source. Also, we had access to the raw evaluation data which were previously used. 

The main characteristics of the three ontologies used for evaluation are summarised in \hyperref[table:dataset_ontologies]{Table~\ref*{table:dataset_ontologies}}. 
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{table}
	\begin{tabularx}{\textwidth}{l c *{3}{Y}}
		\toprule
		\multirow{2}{*}{\emph{Number of}} & \multicolumn{3}{c}{\emph{Ontology}}\\
		\cmidrule(l){2-4} 
		  & Climate Change & Tennis & Finance \\
		\midrule
		 Classes  & 101 & 52 & 77 \\
		 Properties  & 28 & 34 &  29 \\
		 SubClass Relations  & 84 & 35 & 78 \\
		 Individuals  & 64 & 33 & 47 \\
		\bottomrule
	\end{tabularx}
	\caption{Characteristics of the used ontologies}
	\label{table:dataset_ontologies}
\end{table}
\endgroup
Two of these ontologies, covering the domains \emph{climate change} and \emph{tennis}, emerged from seed ontologies used in an ontology learning algorithm~\cite{liu2005semi}. They evolved from several rounds of adding more input data~\cite{wohlgenannt2012}. The other ontology covers the finance domain and represents a small subset of the vocabulary defined by the \gls{eurovoc}\footnote{\url{http://eurovoc.europa.eu/drupal/?q=evontology} accessed 2018/07/13}.

The tested ontologies are of limited size which makes evaluation easier, but still has significance for testing the impact of Context enrichment in ontology validation. Whereas the \emph{Climate Change} ontology contains 101 concepts, 28 object properties, 64 individuals and 84 subclass relations, the \emph{Finance} ontology is of smaller size, containing 77 concepts, 29 object properties, 47 individuals and 78 subclass relations. The \emph{Tennis} ontology has 119 entities in total, 52 of which are concepts, 34 object properties and 33 individuals. 

\subsubsection{Evaluation Setup}
For calculating evaluation metrics the ontologies need to be annotated with reference values. From previous experiments~\cite{wohlgenannt2016} evaluation data was consolidated and annotations were generated. Unfortunately, for some concepts we had ambiguous data or none at all. We manually verified the enriched ontologies by excluding incorrect annotations and adding missing ones where appropriate. This was an important task, particularly because learned ontologies often contain inconsistent and inaccurate data. 

Concerning Crowdsourcing tasks, Figure Eight\footnote{\url{https://www.figure-eight.com/} accessed 2018/07/16}~(former CrowdFlower) allows adjusting a variety of settings. We paid $\$0.05$ per task, required 5 judgements per unit and restricted judgements to the highest quality level of crowd workers~(Level 3). Additionally, we made the assumption that all labels of the validated ontologies are in English, therefore achieving results of higher quality requires restricting participation to the following English speaking countries: Australia, United Kingdom and United States. Furthermore, crowd workers had to correctly answer 8~quiz~questions from politics, computing and tennis in order to qualify for accessing our tasks. Although this does not prevent contributors from randomly answering test questions, it provides at least a minimum of quality control. Without any quality control measures, results would be of little use, as a recent survey reveals~\cite{daniel2018}. 

A central part of the assessment is the definition of evaluation tasks. Crowd workers were consulted to assist in the following ontology engineering task: 

\paragraph{Verification of Domain Relevance} For each selected concept, crowd workers need to decide whether it is relevant for the domain in question~(in our case, Climate Change, Tennis and Finance).
Using domain relevance, we evaluated our proposed methods:
\emph{Ontology~based~Approach}~(\hyperref[sec:neighboring_nodes]{Section~\ref*{sec:neighboring_nodes}}),
\emph{Metadata~based~Approach}~(\hyperref[sec:embedded_context]{Section~\ref*{sec:embedded_context}}) and
\emph{Dictionary~based~Approach}~(\hyperref[sec:external_source]{Section~\ref*{sec:external_source}}).
Each of these generates textual descriptions which were added to the Crowdsourcing task. For the Metadata based Approach we had to manually annotate the ontologies. For the Dictionary based Approach WordNik\footnote{\url{https://developer.wordnik.com/} accessed 2018/06/15} was consulted to provide example sentences. No pre-processing was necessary for the Ontology~based~Approach. 

