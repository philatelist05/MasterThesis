\newacronym{owl}{OWL}{Web Ontology Language}
\newacronym{ace}{ACE}{Attempto Controlled English}
\newacronym{api}{API}{Application Programming Interface}

\section{Conclusion}\label{sec:conclusion}
In this section each research question is revisited and answered by taking into consideration the results from our experimental evaluation~(see \hyperref[chap:results]{Chapter~\ref*{chap:results}}) and drawing final conclusions that point future research in novel directions. 

The main research question examined in this thesis was:
 
\textbf{RQ-I} \emph{Does the crowd perform better on Context enriched Crowdsourcing tasks?}

Answering this question might seem difficult at first because measuring the crowd's performance depends on the metrics of measurement as well as the concrete evaluation settings. However, all proposed methods performed better with regard to F-Measure than in experiments omitting Context. Indeed all our experiments showed that in each dataset the number of correct classifications is considerably higher. 

Clearly, the most important performance metric is F-Measure because it combines the benefits and minimises the drawbacks of Precision and Recall at the same time. We observed that crowd workers tend to decline relevant concepts if they were unsure and Context was either missing or not relevant. Considering our approach is embedded in an ontology learning framework, this seems unproblematic because domain experts and ontology engineers rather prefer deleting a few concepts instead of missing some important ones~\cite{sabou2006}. 

Our experiments that were performed on three datasets including tennis, climate change and finance showed that our approach is feasible and improves the results of the ontology validation process. It was already mentioned~\cite{mortensen2015, mortensen2016, wohlgenannt2016} that crowd-based ontology validation is a good alternative to manual validation, especially in situations where experts are unavailable, budget is limited or the ontology is just too large. 

\textbf{RQ-II} \emph{What methods can be applied that generate Context?}

We measured the performance of the crowd using three methods that generate concept descriptions requiring either manual intervention or being fully automated. All of our proposed approaches are discussed in detail in~\hyperref[chap:context_enrichment_methods]{Chapter~\ref*{chap:context_enrichment_methods}}. 

The Ontology based Approach~(see \hyperref[sec:neighboring_nodes]{Section~\ref*{sec:neighboring_nodes}}) processes hierarchical relations that were encoded within the ontology. The biggest advantage of this approach is that it does not have any external dependencies and works fully automatically. This algorithmic approach is recommended for ontologies containing a large number of concepts that are connected by subsumption relations. A potential pitfall of this method is that the full potential of \gls{ace} could not be leveraged because we identified certain obstacles that hinder the integration of \gls{owl}~Verbalizer, a tool that converts an ontology into a set of \gls{ace} sentences. Consequently, our algorithm generates the text by simple string replacement, not taking the word category~(e.g. singular or plural) into account. 

The second approach~(Metadata based Approach --- see \hyperref[sec:embedded_context]{Section~\ref*{sec:embedded_context}})
is based on metadata that resides within the ontology. In contrast to the other methods this approach requires some manual work. As a precondition 
the metadata needs to be added by experts in a standardised format which served then as Context in Crowdsourcing tasks. Because the additional costs of manual preprocessing might not outweigh the benefits of high quality concept descriptions, it makes sense to preferably use this approach in very specialised areas such as in biomedical domains where ontologies are typically well documented and already contain explanations.

The idea of the Dictionary based Approach~(see \hyperref[sec:external_source]{Section~\ref*{sec:external_source}})
is that starting from a concept name, descriptions are built from consulting an online dictionary. WordNik was chosen as the provider of concept descriptions. These are formed from example sentences that contain the requested concept name. This approach has its strengths when concepts are relatively generic. We also noticed that the lookup failed when concept names contained special characters or had multiple meanings associated. 
To conclude, this approach is rather simple and easy to implement, however, it may have the potential to generate wrong results, especially for ambiguous concepts. 


\textbf{RQ-III} \emph{To what extent is it possible to transfer the investigated methods to different datasets?}

Unfortunately, none of our proposed methods can be applied in all contexts. Each method has its own prerequisites:

The Ontology based Approach highly depends on the ontology structure because it processes subsumption relations. The algorithm fails for flat hierarchies that contain little or no subsumption relation. However, this restriction seems reasonable because limiting our viewpoint to subsumption relations was caused by major obstacles that prevented the integration of \gls{owl}~Verbalizer, a tool which also takes other relation types into account. 

At the core of the Metadata based Approach are annotations that reside within the ontology. Unfortunately, none of our evaluated ontologies contained these metadata by nature which required us to add them by hand. We think though, that this requirement is rather feasible because our experiments showed that this approach generates concept descriptions of high quality.  In fact, in very specialised areas such as in biomedical domains ontologies already contain such explanations. 

For the Dictionary based Approach the situation is different because the design of the algorithm does not impose any restrictions on the internals of the validated dataset. The outcome rather depends on the responses from conducting the online dictionary \hyperref[sec:wordnik]{WordNik}. We observed two peculiarities that may be considered when choosing a dataset:
\begin{inparaenum}[i)]
	\item the lookup failed when concept names contained special characters~(e.g. quotes), and
	\item the response contained irrelevant example sentences when the meaning of the requested concept is ambiguous.
\end{inparaenum}


\textbf{RQ-IV} \emph{Which of the proposed methods work best? What are potential shortcomings and why?} 

Based on the results presented in~\hyperref[chap:results]{Chapter~\ref*{chap:results}}, the Metadata~based~Approach outperformed all other methods even though it was only ranked second in terms of F-Measure for the Climate Change dataset. A clearer picture can be drawn when looking at the combined statistics showing the level of agreement and the judgment's accuracy. In all these rankings this approach was ranked as the best performing method. This outcome was rather expected because, compared to the other approaches, concept descriptions were of highest quality while keeping the number of missing explanations at zero. Indeed, allowing expert participation produces qualitative results but is also very costly.  

In contrast to our expectations the Dictionary~based~Approach had the most problems in finding relevant descriptions. It performed even worse than omitting descriptions in the tennis dataset. This has several reasons: From analysing the traffic of WordNik consultation, we know that some of the responses were irrelevant or were even missing for certain concepts. This is certainly true for concepts having special characters~(e.g. quotes) in their names or concepts which have multiple meanings associated. 

One restriction that should be considered especially when validating large ontologies containing several hundreds or even thousands of concepts is that WordNik limits the number \gls{api} calls/requests in the basic setup. However, they offer paid plans\footnote{\url{https://developer.wordnik.com/pricing} accessed 2019/02/03} to users who need more calls or more data.

A common phenomenon over all datasets was the relatively high Recall, even when omitting concept descriptions at all. This means that the crowd predominately declined relevant concepts, however, as mentioned earlier, this is relatively unproblematic because domain experts and ontology engineers rather prefer deleting a few concepts instead of missing some important ones. 

Surprisingly, the Ontology~based~Approach worked pretty well even though its performance could not reach the top ranked method. Unquestionably,
the quality of the descriptions directly correlates to the number of subsumption relations. This works especially for learned ontologies because learning frameworks naturally create ontologies with deep hierarchies. But since all concepts were connected by subsumption relations, explanations were missing for those isolated concepts. 

