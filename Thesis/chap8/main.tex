\chapter{Discussion \& Conclusion}\label{chap:discussion_and_conclusion}
% Describe briefly what this thesis is about
% Relate to chapter2 (e.g. related work) - context and approaches in particular
% Desribe then what this thesis tries to solve (e.g. briefly describe each of the 3 methods )

In the remainder of this chapter each research question is revisited by taken together all the results from the experimental evaluation~(see \hyperref[chap:results]{Chapter~\ref*{chap:results}}) and drawing final conclusions that point future research in new directions. 

The main research question examined in this thesis was:
 
\textbf{RQ-I} \emph{Does the crowd perform better on context enriched Crowdsourcing tasks?}

Answering this question might seem difficult at first because measuring the crowd's performance depends on the metrics of measurement as well as the concrete evaluation settings. However, all proposed methods performed better with regard to F-Measure than omitting Context. Indeed all our experiments showed that in each dataset the number of correct classifications with added Context is considerably higher. 

Clearly, the most important performance metric is F-Measure because by combining Precision and Recall it strengthens the benefits as well as weakens the shortcomings of both metrics. We observed that the lead of the methods with Context is slightly lower when measured by recall compared to precision. In other words, crowd workers tend to rather decline relevant concepts. Considering that our approach~(e.g. ontology validation) is embedded in a bigger ontology learning process, this seems unproblematic because domain experts and ontology engineers rather prefer deleting a few concepts rather than missing some important ones~\cite{sabou2006}. 

Based on our experiments performed on three datasets including tennis, climate change and finance we proposed a viable solution that adds Context to Crowdsourcing tasks and improves the results of the ontology validation process. It has already been mentioned~\cite{mortensen2015, mortensen2016, wohlgenannt2016} that crowd-based ontology validation is a good alternative to manual validation, especially in situations where an expert is unavailable, budget is limited or the ontology is just too large. 

\textbf{RQ-II} \emph{What methods can be applied that generate Context?}

We measured the performance of the crowd using three methods that generate concept descriptions either requiring manual intervention or being fully automated. Our proposed approaches were discussed in detail in~\hyperref[chap:context_enrichment_methods]{Chapter~\ref*{chap:context_enrichment_methods}}). 

The Ontology based Approach~(see \hyperref[sec:enrichment_ontology_approach]{Section~\ref*{sec:enrichment_ontology_approach}})) processes hierarchical relations that are encoded within the ontology. The biggest advantage of this approach is that it works without any external dependencies in a fully automated manner. This algorithmic approach is recommended for ontologies containing a large number of concepts that are connected by subsumption relations. A potential pitfall of this method is that the full potential of ACE~(Attempto Controlled English) could not be leveraged because we identified certain obstacles that hinder the integration of OWL~Verbalizer, a tool that converts an ontology into a set of ACE sentences. Consequently, our algorithm generates the text by simple string replacement, not taking the word category~(e.g. singular or plural) into account. 

The second approach~(Metadata based Approach --- see \hyperref[sec:enrichment_metaData_approach]{Section~\ref*{sec:enrichment_metaData_approach}})
is based on metadata that is encoded within the ontology. In contrast to the other methods this approach requires some manual work. As a precondition 
the metadata needs to be added by experts in a standardised format which is then provided as Context in Crowdsourcing tasks. Because the additional costs of manual preprocessing might not outweigh the benefits of high quality concept descriptions, it makes sense to preferable use this approach in very specialised areas such as in biomedical domains where ontologies are typically well documented and already contain explanations.

The idea of the Dictionary based Approach~(see \hyperref[sec:enrichment_dictionary_approach]{Section~\ref*{sec:enrichment_dictionary_approach}}))
is that starting from a concept name, descriptions are built from consulting an online dictionary. \hyperref[sec:wordnik]{WordNik} was chosen as provider of concept descriptions which are formed from example sentences that contain the requested concept name. This approach has its strengths in situations where ontologies are validated that cover novel domains or were created from sources that arise just recently. We also noticed that the lookup failed when concept names contained special characters or their multiple meanings were associated. 
To conclude, this approach is rather simple and was easy to implement, however, it may have the potential to generate wrong results, especially for ambiguous concept names. 


\textbf{RQ-III} \emph{To what extent is it possible to transfer the investigated methods to different datasets?}

Unfortunately, none of our proposed methods can be applied in all contexts. Each method has its own prerequisites:

The Ontology based Approach is highly influenced by the structure of the ontology because it processes subsumption relations. The algorithm fails 
for flat hierarchies containing little or no parent-child relations. However, this restriction seems reasonable because limiting our viewpoint to subsumption relations was caused by some obstacles the hindered the integration of OWL~Verbalizer, a tool which also takes other relation types into account. 

At the core of the Metadata based Approach are the processing of annotations that reside within the ontology. Unfortunately, none of our evaluated ontologies contained these metadata by nature which required us to add them by hand. We think though, that this requirement is rather feasible because our experiments showed that this approach outputs concepts descriptions of high quality.  In fact, in very specialised areas such as in biomedical domains ontologies are already contain such explanations. 

For the Dictionary based Approach the situation is different because the design of the algorithm does not impose any restrictions on the internals of the validated dataset. The outcome rather depends on the responses from conducting the online dictionary \hyperref[sec:wordnik]{WordNik}. We observed two peculiarities that may be considered when choosing a dataset:
\begin{inparaenum}[i)]
	\item the lookup failed when concept names contained special characters~(for example quotes), and
	\item the response contained irrelevant example sentences when the meaning of the requested concept is ambiguous.
\end{inparaenum}


\textbf{RQ-IV} \emph{Which of the proposed methods work best? What are potential shortcomings and why?} 

Based on the results presented in~\hyperref[chap:results]{Chapter~\ref*{chap:results}}, the Metadata~based~Approach outperformed all other methods even though it was only ranked second in terms of F-Measure for the Climate Change dataset. A cleared picture can be drawn when looking at statistics combining the level of agreement and the judgment accuracy. In all these rankings this approach was ranked as the best performing method. This outcome was rather expected because, compared to the other approaches, concept descriptions were of highest quality while keeping the number of missing explanations at zero. Indeed, allowing expert participation produces qualitative results but is also very costly.  

In contrast to our expectations the Dictionary~based~Approach has the most problems finding relevant descriptions. It performed even worse than having no Context at all when experiments used the tennis dataset. This has several reasons: From analysing the traffic of WordNik consultation, we know that some of the responses were irrelevant or were even missing for certain concepts. This is certainly true for concepts having special characters~(e.g. quotes) in their names or concepts which have multiple meanings associated. 

One restriction that should be considered especially when validation large ontologies with several hundreds or even thousands of concepts is that WordNik limits the number API calls/requests in the basic setup we used. However, they offer paid plans\footnote{\url{https://developer.wordnik.com/pricing} accessed 2019/02/03} to users who need more calls or more data.

A common phenomenon over all datasets was the relatively high number of Recall, even when omitting concept descriptions at all. This means that the crowd predominately declined relevant concepts, however, as mentioned earlier, this is relatively unproblematic because domain experts and ontology engineers rather prefer deleting a few concepts rather than missing some important ones. 

Surprisingly, the Ontology~based~Approach worked pretty well even though its performance could not reach the top ranked method. Unquestionably,
the quality of the descriptions directly correlates to the number of subsumption relations in an ontology. This works especially for learned ontologies as we had because learning frameworks naturally ontologies with deep hierarchies. But not all concepts were connected by subsumption relations, explanations were missing for those isolated concepts. 


