\section{Motivation}
The advance of embedding Information Technology in all kinds of electronic devices and connecting them to collect and exchange data imposes new challenges of handling the increasing amount of data. Although many problems can be solved by machines only, there are certain tasks where humans perform better than computers. In \emph{Crowdsourcing}, collective human intelligence~(the crowd) is used to solve these complex tasks. \cite{yuen2011}~grouped crowdsourcing applications in 
\begin{inparaenum}[1)]
		\item Voting Systems,
		\item Information Sharing Systems,
		\item Games with a purpose~(GWAP) Systems and
		\item Creative Systems.
\end{inparaenum} 
First, Voting Systems like Amazon Mechanical Turk~(MTurk)\footnote{\url{https://www.mturk.com/}} use majority voting to consider the answer with the highest number of votes as the correct one. Second, Information Sharing Systems enable users sharing and distributing knowledge among the crowd. Third, Games with a purpose~(GWAP) Systems facilitate playing small games in order to solve some meaningful tasks. Fourth, Creative Systems include tasks like labelling an image, writing algorithms or editing text. 

An inherent factor of the Semantic Web is its large amount of Linked Data~(e.g. DBpedia~\cite{lehmann2015}). Semantic technologies have emerged in various areas including domain modelling, data integration, enhanced search and content management~\cite{semantic-web-usecases}. Managing Semantic Web tasks is considered resource intensive and often requires human involvement due to its knowledge intensive and context specific nature. On the other side Crowdsourcing applications solve simple and small tasks~(microtasks) in a cost-effective way. \cite{sarasua2015crowdsourcing} summarises major research challenges and opportunities in combining Crowdsourcing and Semantic Web technologies. The most important challenges include 
\begin{inparaenum}[1)]
		\item task and workflow design,
		\item managing the quality of contributions,
		\item handling multiple Crowdsourcing genres and 
		\item finding and managing the right crowd.
\end{inparaenum}
Whereas research shows that breaking tasks into smaller pieces and formulating the right questions has a huge impact on the outcome of Crowdsourcing tasks, it is equally important to establish a model which formally defines the required quality and skills to solve tasks. Also, there exist no general guidelines when and under which circumstances preferring small crowds with domain experts over large crowds with less qualified crowd workers is better. However, \cite{mortensen2013} concluded that average crowds perform on par with domain experts in "common sense" application domains, if crowd workers are carefully selected by qualification tests and tasks are presented in the simplest possible form. Although their experiments revealed that solving domain specific tasks (e.g. classification of diseases) requires good context and background knowledge, it was not further investigated how contextual data can be added in an automated manner. 

This work investigates which contextual data is needed to get better results, what would be the improvements in the overall workflow and how does the architecture of a concrete implementation look like. To answer the second question, a detailed evaluation on selected domains and ontologies is conducted. 

As a practical part, an extension of the existing \emph{uComp~Protege~Plugin~\cite{wohlgenannt2016}} is implemented. This plugin uses "embedded crowdsourcing" to verify properties of generic ontologies. In particular, it supports verification of Relation Correctness~(T2), Relation Type~(T3) and Domain Relevance~(T4). It achieves good results but their quality could be improved if concepts were better documented with relevant context. 
