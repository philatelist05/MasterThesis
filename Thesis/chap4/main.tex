\chapter{Experimental Evaluation}
In this chapter we describe our approach to evaluate the performance of crowdsourced ontology validation steps. More precisely, in \hyperref[sec:evaluation_metrics]{Section~\ref*{sec:evaluation_metrics}} we start by describing all relevant performance metrics used to quantify the improvements. Then, in \hyperref[sec:evaluation_datasets]{Section~\ref*{sec:evaluation_datasets}} an overview of the used datasets~(e.g. ontologies) is given. 

\subsubsection{Evaluation Hypothesis}
Based on existing efforts for ontology validation using crowdsourcing~(see \hyperref[sec:ucomp_protege_plugin]{Section~\ref*{sec:ucomp_protege_plugin}}), we formulate the following evaluation hypothesis:
\begin{quotation}
	The crowd performs ontology validation steps better if context is added to crowdsourcing tasks.
\end{quotation}

To evaluate the hypothesis stated above, we performed experimental evaluations on the implemented Protege~plugin. \hyperref[table:overview_exp_evaluation]{Table~\ref*{table:overview_exp_evaluation}} gives an overview of the experiments including their settings and used datasets, described thoroughly in the next sections. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION: EVALUATION METRICS %
\input{section_evaluationMetrics.tex}

% SECTION: DATASETS %
\input{section_datasets.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
