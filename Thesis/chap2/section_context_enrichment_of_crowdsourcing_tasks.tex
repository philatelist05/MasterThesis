\section{The use of Context in Crowdsourcing Tasks}
% Notes from meeting with MS:
% 
% -) In related work, consider the use of context (e.g. define what "Context" means --> see definition from MS)
% -) Use table with references provided by MS

\subsection{Context}\label{sec:context_in_crowdsourcing_tasks_context}
When analysing the use of context in crowdsourcing tasks, we noticed that there exists no formal definition of the term \guillemotright context\guillemotleft~. In fact, all approaches that were found use a different notion of that term. 
This section first investigates what context definitions these approaches use. After that, we give a consolidated definition that fits our approach of crowd-based ontology validation.

When investigating the use of context in crowdsourcing tasks, a good start is to look at~\cite{sarasua2015crowdsourcing}. In this work, the authors did an extensive literature study to find challenges in the context of Crowdsourcing and the Semantic Web. One of the challenges they found was a proper definition of context as part of a complete task design. Concretely, they asked but did not answer the minimum required context a crowd needs to finish a task correctly. Unfortunately, during our studies we could not find an answer either. It seems that there exists no generic answer which applies in all contexts, it rather depends on the concrete type of task that needs to be solved. 

During our literature research we found that approaches can be categorised as tasks supplying \textbf{explicit context}, tasks supplying \textbf{implicit context} and those providing \textbf{no context} at all. 

The most obvious work supplying \emph{no~context} with crowdsourcing tasks was actually done by~\cite{wohlgenannt2016}. It represents the baseline of our work and motives our use of context. A detailed explanation of this paper is out of scope for this section. However, a detailed explanation was already done in~\hyperref[sec:ucomp_protege_plugin]{Section~\ref*{sec:ucomp_protege_plugin}}. In another paper, a method of collaborative ontology construction was proposed~\cite{zhitomirsky2017}. The actual definition of the ontology was implemented by a hybrid approach containing the definition of RDF-triples by non-experts~(e.g. students) and their classification by the crowd.

Clearly, the omission of context does not need to be problematic. Whereas crowd-based ontology validation without context clearly has its drawbacks, it would not be beneficial if the crowd had additional information in the ontology construction example because the entities that formed the statements that were judged were simple and easy understandable by the crowd. 

The other group of tasks that provide additional information are those tasks supplying \emph{explicit~context}. Similarly to our approach the authors of \cite{mortensen2015} and \cite{mortensen2016} supplied concept descriptions to improve the quality of the judgements. Their goal was to find inconsistencies and errors in SNOMED~CT, a widely used ontology mainly used in biomedical contexts. Even though biomedical ontologies are well documented, not all entities have definitions. For that, English language definitions were manually added by domain experts. 

The last category contains tasks with \emph{implicit~context}, meaning that context was not intentionally added. Context is rather defined implicitly, e.g. all context is already present in the initial dataset. Hence, no additional process or algorithm is needed to define contextual information. For example, in~\cite{acosta2018} the authors used a crowdsourcing data quality assessment tool to detect errors in Linked Data. 
For their analysis they used DBPedia~\cite{auer2007} as evaluation source. Because one of the design principles of DBPedia was to derive linked information from Wikipedia\footnote{\url{https://www.wikipedia.org/}}, it seems natural to add the link to the corresponding Wikipedia page to the crowdsourcing task interface. To that end, no additional process or method for the enrichment of context exists because the evaluated dataset already contains the context.

A different approach was taken by~\cite{sabou2018, winkler2017, winkler2017_2} in which an Extended Entity Relationship~(EER)~diagram was verified against a software specification document in a software engineering use case. The diagram, initially created by students, was presented together with the specification text to detect and correct inconsistencies in the conceptual model. From the task description it seems obvious that no additional information is needed because context was already given implicitly by the EER~diagram. In that sense, the context was defined in terms the specific task that was carried out by the crowd. 

Taken together all insights from above, we define \guillemotright context\guillemotleft~in crowdsourcing tasks as:

\begin{defn}
	\emph{(Context)} Context refers to any sort of additional information that is supplied with a crowdsourcing task to improve its understanding in
	such a way that it positively affects the crowds performance and the result quality. Furthermore, we do not set a limitation on the type or format 
	of context that is provided. Examples are natural language descriptions, links to external content or pictures. We distinguish between
	crowdsourcing tasks that
	\begin{inparaenum}[1)]
			\item supply explicit context,
			\item those that supply context implicitly and
			\item those that provide no context at all.
	\end{inparaenum}
\end{defn}

\subsection{Approaches that Context in Crowdsourcing Tasks}
Based on the definition of \guillemotright context\guillemotleft~in~~\hyperref[sec:context_in_crowdsourcing_tasks_context]{Section~\ref*{sec:context_in_crowdsourcing_tasks_context}}, in this section we give an overview of various approaches that use~(or omit) context in crowdsourcing tasks.
\hyperref[table:approaches_of_context_in_crowdsouring_tasks]{Table~\ref*{table:approaches_of_context_in_crowdsouring_tasks}} summarises all approaches that were discussed in this section.

\begingroup
\renewcommand{\arraystretch}{2.5}
\begin{table}
	\begin{tabularx}{\textwidth}{X l*{4}{Y}}
		\toprule
		Paper & Evaluated Unit & Evaluation Target & Context Type & Context \\
		\midrule
		\cite{acosta2018} & RDF triples & Data Quality & Implicit Context & Wikipedia Link \\
		\cite{mortensen2015, mortensen2016} & Ontology Structure & Data Quality & Explicit Context & Concept Descriptions \\
		\cite{sabou2018, winkler2017, winkler2017_2} & Conceptual Model & Data Quality & Implicit Context & EER Diagram \\	
		\cite{wohlgenannt2016} & Ontology Structure & Data Quality & No Context & No Context \\			
		\cite{zhitomirsky2017} & RDF triples & Data Definition & No Context & No Context \\
		\bottomrule
	\end{tabularx}
	\caption{Overview of approaches that context in crowdsourcing tasks}
	\label{table:approaches_of_context_in_crowdsouring_tasks}
\end{table}
\endgroup

The first approach~\cite{acosta2018} investigates quality issues that along the two dimensions accuracy and interlinking. The attributes that were evaluated by the crowd were incorrect object values, incorrect datatypes or language values and incorrect links. The evaluation was performed on a linked dataset extracted from the DBPedia corpus. The quality assessment consisted of a twofold approach. In the first stage, a group of Linked Data experts had to select possible candidates of RDF-triples that might have quality problems. In the second stage, the triples were evaluated by the crowd. Because DBPedia triples were constructed by knowledge extraction from Wikipedia, the link to the corresponding Wikipedia page served as context that was shown on crowdsourcing task interfaces. Even though the results were promising, the full potential could possibly be reached by a hybrid approach which combines crowd-based evaluation with an automatic process that helps to reduces the number of triples that resort to crowdsourcing. 

The next work was initially proposed in~\cite{mortensen2015} and then extended by~\cite{mortensen2016} to verify the quality of hierarchical relations in biomedical ontologies. To save costs, they selected a random subset of 200 subsumption relations from SNOMED~CT, an ontology that often serves as knowledge reference in biomedical contexts. For the evaluation, the crowdsourcing task was generated from subsumption relations and concept descriptions. Due to the complexity of application domain, biomedical ontologies are well documented and contain concept descriptions. Those concepts with missing descriptions were enriched with documental information. The authors concluded that the crowdsourcing results were comparable to the manual evaluation done by medical experts, however, for certain tasks, especially more complex ones that are poorly documented, should be better done by domain experts. 

A couple of researchers~\cite{sabou2018, winkler2017, winkler2017_2} investigated conceptual model verification from a Software Engineering perspective. They used crowdsourcing techniques to verify the correctness of Extended Entity Relationship~(EER)~diagrams. In the first experiment, students had to create the conceptual model~(EER-diagram) from a specification document which was written in informal English language. The resulting models~(diagrams) were then checked by the crowd to identify inconsistencies between the model and the specification text. In this setting, the EER-diagram served as context for each individual crowdsourcing task. Their experiments achieved high precision and recall, however a few shortcomings will be addressed in future revisions. 

While all the crowdsourcing tasks discussed so far had contextual information to some extend, the approaches presented next completely omit context. 
One of these approaches represents the baseline of our work and motives our use of context~\cite{wohlgenannt2016}. A detailed explanation was already done in~\hyperref[sec:ucomp_protege_plugin]{Section~\ref*{sec:ucomp_protege_plugin}}.

The last paper that is discussed in this section is~\cite{zhitomirsky2017}. It covers the process of ontology construction, a costly and time consuming task that involves extensive expert participation. In this work, the authors presented a two-step approach. It consists of collaborative ontology construction by non-experts and classification of statements that were formed from RDF-triples. Special attention was paid towards the reduction or omission of subjective or biased judgements. For that, multiple viewpoints were merged to create a unified multi-viewpoint ontology.
Whereas the initial task of collecting controversial subjects and creating multiple single-viewpoint ontologies was done manually by non-professionals, the classification the construct one unified multiviewpoint ontology was performed by the crowd. 
The results showed that no additional context is needed if the domain not too complex and the statements~(relations) were easily understandable.  


